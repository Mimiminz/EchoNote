# 아이디어 구체화

날짜: 2024년 8월 30일
상태: 진행 중

# 한 줄 소개

### 번역 더빙 제공 및 유튜브 업로드 연동 서비스

---

## ❔ WHY

- **❗이용 대상에 따라 수정 필요❗**
- 다양한 국가의 시청자를 보유하고 있는 유튜버는 번역된 자막을 제공한다.
- 단순 자막 대신 방송인의 목소리로 더빙된 오디오를 제공한다면 시청자의 만족도가 더 높아지지 않을까?
- 방송인의 오디오 → STT → 번역 → TTS → RVC를 거쳐 더빙된 오디오를 생성
- 유튜브에서 더빙된 음성을 넣으면 자막처럼 더빙 사운드로 변경 가능
  - 더빙된 음성 파일을 제공하고, 유튜브 영상 업로드 기능과 함께 편리한 서비스 제공
  - 서비스 제공자 입장 : 손쉽게 더빙된 다양한 언어를 제공할 수 있다.
  - 시청자의 입장 : 더빙된 영상을 통해 영상을 쉽게 이해할 수 있다
    - 제공자의 구독자 상승 효과
- 자막이 아닌 더빙된 음성을 들었을 때 이익이 되는 대상
  - 화면과 자막을 동시에 봐야하는 기술 관련 유튜버를 대상으로 하면 수요가 많을 것으로 예상

---

## 🎯 이용 대상

- 정보를 얻기위한 사람들을 시청자로 가지는 유튜버 등 동영상 업로더
  - 기술 튜토리얼 같은 영상이 외국어면 자막을 보면서 따라하기가 힘듦

---

- 이용 주체는 영어를 사용하는 해외 크리에이터 ( 주체는 영어권으로 지정. 이후 확장 예정 )
- **코딩 관련 기술 설명 해외 유튜버**의 영상을 대상으로
  - 코딩으로 한정지은 이유
    - 전문 용어 번역 처리 때문에 (예: index, insert 등….)
  - 나중에 서비스 확장을 통해 다양한 카테고리 더빙 기능 제공 가능
- 영어 → 한국어로 번역된 영상 제공
  - 유튜버 본인의 음성으로
  - 우리 서비스의 기본 언어는 영어
  - 이것도 이후 서비스 확장 시 다양한 기능 제공 가능

---

### 카테고리 한정 이유

- 저챗에 적용하기 어려운 이유
  - 자유롭게 말한다 → 문장이 딱딱 떨어지지 않음
  - 노이즈
    - 정보 제공을 목적으로 하는 기술 설명 유튜브와는 달리 자유롭게 말하는 형식은 비교적 노이즈가 발생할 것으로 예상됨

---

## 요구사항 리스트

### 로그인 및 회원가입

- ~~자신의 영상 목록 관리 목적~~
  - 우리가 굳이 생성된 영상을 다 관리해줘야 하나?
  - RVC 학습에 필요한 음성을 보관 및 수정
    - 음성을 보관하지는 않고 학습된 모델만 사용자별로 갖고 있으면 될듯
    - 학습용 음성 관리 기능은 나중에~~
    - 개인별 모델 성능 향상은 나중에 생각하기…
- 크리에이터의 목소리 학습을 관리해야 함
  - 우리가 학습을 위한 가이드 라인을 제공해야 함
  - 10분 이상의 음성 업로드 필수
- 유튜브 계정 연동
  - 영상 + 더빙된 음성 한 큐에 자동 업로드
    - 영상 업로드는 api 있는 걸로 확인됨
    - 다만 음성 자막 업로드는 자막을 작성하는 부분에 음성 파일을 올려야 함
      - api에서 제공하는지 더 알아볼 필요 有
    - 음성까지 업로드가 불가능 할 경우 영상만 업로드하고 유튜브 주소를 제공
      - 음성파일 다운로드 기능. 업로드를 원할 경우 제공한 주소(유튜브)에서 추가 등록
      - 번역된 자막도 같이 제공**`~.~`**
  - (추후 기능) 타임스탬프 생성이나 썸네일 수정 등
    - 요거의 목적은 우리 서비스랑 유튜브를 번갈아 이동하는 번거로움을 줄이는 것
    - 타임스탬프는 영상 설명에만 작성하면 되니까 이 api가 있으면 가능할듯

### 영상 업로드

- 음성 분리 → STT로 변환 → 문장별 타임스탬프 추출 → TTS로 번역된 문장 생성 → RVC로 더빙된 오디오 생성 → 싱크 맞춰서 → 완성본 제공

### 스크립트 수정 기능

- 원문에 대한 스크립트 수정 기능 제공
- 크리에이터의 말을 제대로 인식했는지 확인하는 목적

---

## 산출물

- 음성만 따로 주기도 하고
- 영상도 주기도 하고
- 미리 듣기/보기도 제공해야 할듯

---

## ⚠️ 예상 문제점

### 1) 번역 퀄리티

- 실시간 방송이 아닌 이미 업로드된 영상을 대상으로 하면 어느정도 해소 가능

### 2) 언어별 길이 차이

- 단어의 길이가 길어져 와다다 말하게 될 수 있다.

### 3) 잡음 제거

- 이미 완성된 영상 콘텐츠를 대상으로 하면 배경음, 효과음으로부터 방송인의 목소리를 잘 분리해야 함
- 실시간 방송을 대상으로 한다면 마이크에 들어오는 목소리만 이용하면 됨

---

## 조사할 것들

- 음성 분리 (효과음, 배경음, 기타 노이즈 분리) `현주`
  [DeepFilterNet](https://github.com/Rikorose/DeepFilterNet)
  샘플 데이터: [https://rikorose.github.io/DeepFilterNet2-Samples/sample2/](https://rikorose.github.io/DeepFilterNet2-Samples/sample2/)
  사용해본 후기
  - 실시간으로 음성 노이즈 제거 가능
  - 노이즈 제거 가능
- STT `다솔`
  - [Whisper](https://github.com/openai/whisper)
    - 특징
      - 영어 전용 모델 있음
      - 근데 tiny.en이랑 base.en 모델 외에는 다국어 모델과 큰 차이가 있지 않음
    - 장점
      - 다양한 확장자(mp3, mp4, m4a, mpeg, mpeg, wav 등) 지원
      - 다국어 존재(확장성 고려)
      - 정보가 많음
      - 발화에 따른 타임스탬프를 가져올 수도 있을거같음
        [GitHub - linto-ai/whisper-timestamped: Multilingual Automatic Speech Recognition with word-level timestamps and confidence](https://github.com/linto-ai/whisper-timestamped)
        [여기서 뭔가 타임스탬프를 리턴받음](https://m.blog.naver.com/jmc0820/223134425566)
    - 단점
      - [이슈](https://www.bytesizeponderings.com/whisper-review-2)
  - [DeepSpeech](https://github.com/mozilla/DeepSpeech)
    - 특징
      - 영어로 사전 학습되어 있음
      - 추가 학습 가능
    - 장점
      - Python 외에도 C, Java, Javascript에서 사용 가능?
    - 단점
      - Mozilla에서 개발 종료함 최신 릴리즈가 2020
  - [Kaldi](https://kaldi-asr.org/doc/)

    학습 시켜야하는 거 같은데 봐도봐도 모르겠음…

    [GitHub - kaldi-asr/kaldi: kaldi-asr/kaldi is the official location of the Kaldi project.](https://github.com/kaldi-asr/kaldi)

    [Kaldi Tutorial](https://www.eleanorchodroff.com/tutorial/kaldi/introduction.html)

  - [~~SpeechBrain~~](https://github.com/speechbrain/speechbrain)
    - ~~특징~~
      - ~~오픈소스 파이토치 툴킷~~
      - ~~음성 인식, 화자 인식, Speech Separation, Speech Enhancement 등등등~~
      - [~~사전학습모델~~](https://huggingface.co/speechbrain)
    - ~~장점~~
    - 단점
      - 찾다보니까 STT가 없는거같음
      - 아닌가
- TTS `민주`
  아래 TTS는 따로 한국어 데이터셋을 추가하고 훈련하는 과정 필요.
  주로 https://www.kaggle.com/datasets/bryanpark/korean-single-speaker-speech-dataset을 이용해서 학습함
  1. https://github.com/mozilla/TTS
     1. 딥러닝 기반 Tacotron2 모델 사용. 사용자가 TTS 모델을 훈련시킬 수 있도록 지원
  2. https://github.com/ming024/FastSpeech2
     1. 더 빠른 합성 속도. 적은 데이터로도 효과적인 TTS 생성 가능
  3. https://github.com/NVIDIA/tacotron2
     1. NVIDIA의 Tacotron2 구현체. GPU 가속을 활용해 TTS 합성 속도 상승. 사람같은 발음 및 음성 생성 가능
  https://colab.research.google.com/drive/1ybWwOS5tipgPFttNulp77P6DAB5MtiuN?usp=sharing
  https://github.com/seastar105/kr-custom-tts : colab 또는 GPU로 학습하는 모델
  https://github.com/hccho2/Tacotron2-Wavenet-Korean-TTS
  다른 한국어 tts 모델 학습한 것 보면 KSS 데이터 셋을 가장 많이 쓰고
  https://program.kbs.co.kr/2fm/radio/uvolum/pc/index.html 이런 방송 데이터 셋도 학습 시키는 듯
  https://github.com/esoyeon/KoreanTTS?tab=readme-ov-file
  https://github.com/carpedm20/multi-speaker-tacotron-tensorflow
  → 굉장히 영어같아 보이지만 놀랍게도 tacotron의 한국어 구현 깃헙
  → 아마 이게 그나마…..제일 나을듯….?
- 번역 모델 (영 → 한) `지연`
  https://www.deepl.com/ko/pro-api
  - 한 달 50만자까지 번역
  - [8분짜리 영상](https://www.youtube.com/watch?v=ThGbP9wgkz8)에서 공백을 제외한 글자수가 5611자 나왔어용
  https://github.com/vEduardovich/dodari.git
  - 로컬에서 txt, srt 파일을 통해 바로 번역할 수 있는 모델
  - 로컬 웹서비스 형태로 제공된다고 하는데 모델도 다운로드 되니까 요거를 그냥 쓸 수 있는지 알아봐야 함
  https://huggingface.co/MLP-KTLim/llama-3-Korean-Bllossom-8B
- 영상에서 음성 타임스탬프 찍기 `예헌`
  ### 문장 분리
  - 영어 문장 토큰화 - NLTK
    [02-01 토큰화(Tokenization)](https://wikidocs.net/21698)
  - ClarityNLP
    [Sentence Tokenization — ClarityNLP documentation](https://claritynlp.readthedocs.io/en/latest/developer_guide/algorithms/sentence_tokenization.html)
  ### 음성과 문장 비교 후 타임스탬프
  - **Forced Aligner 모델**
    [GitHub - pettarin/forced-alignment-tools: A collection of links and notes on forced alignment tools](https://github.com/pettarin/forced-alignment-tools)
  - Montreal Forced Aligner - Python
    - 가장 많이 사용된다고 함
    - TextGrid 형식으로 반환 → 각 문장의 시작과 끝 타임스탬프 저장
    [Montreal Forced Aligner](https://www.kaggle.com/code/davidnguyens12/montreal-forced-aligner/notebook)
  - Gentle Forced Aligner - Docker
    - 주로 영어 지원
    - 웹 기반 인터페이스 존재
    - JSON 형식으로 반환
    [GitHub - lowerquality/gentle: gentle forced aligner](https://github.com/lowerquality/gentle)
  - Aeneas Forced Aligner - Python
    - 오디오북 같은 긴 텍스트에 잘 동작
    - JSON 형식으로 반환
    [GitHub - readbeyond/aeneas: aeneas is a Python/C library and a set of tools to automagically synchronize audio and text (aka forced alignment)](https://github.com/readbeyond/aeneas)
  - wav2vec2
    [wav2vec2을 이용한 강제 정렬](https://tutorials.pytorch.kr/intermediate/forced_alignment_with_torchaudio_tutorial.html)
  ⇒ **각 라이브러리 별 특징 및 성능 비교 필요**
- 음성과 영상 싱크 맞추기 `은우`
  - 끊은 더빙 문장과 원본 문장 타임스탬프 매핑
  - 타임스탬프 맞춰 더빙 문장을 하나의 음성 파일로 재구성

  - pydub
    [https://wikidocs.net/226774](https://wikidocs.net/226774)
    [GitHub - jiaaro/pydub: Manipulate audio with a simple and easy high level interface](https://github.com/jiaaro/pydub)
    음성의 속도를 조절하는 라이브러리
    여러 음성 이어붙이기도 가능
  - [**Auto-Synced-Translated-Dubs**](https://github.com/ThioJoe/Auto-Synced-Translated-Dubs)
    [GitHub - ThioJoe/Auto-Synced-Translated-Dubs: Automatically translates the text of a video based on a subtitle file, and then uses AI voice services to create a new dubbed & translated audio track where the speech is synced using the subtitle's timings.](https://github.com/ThioJoe/Auto-Synced-Translated-Dubs)
    이미 자막 파일이 있다고 가정. 제일 큰 문제인 음성 길이 문제는 속도를 조절해서 해결
- ~~유튜브 음성 업로드 API?~~
  - 영상 업로드 API 있음
  - 자막 업로드 API 있음
    - 오디오 업로드가 될지는 모름

---

## 💬 다른 의견

### 1) 다시보기에 더빙 제공

- 라이브에서 발생할 수 있는 문제를 해결 가능

### 2) 크롬 익스텐션

- 이용 주체는 시청자
- 영상에 대해 크롤링, 저작권 문제 존재

### **3) 영상 업로드 웹 사이트**

- 본인의 저작물을 본인이 업로드하므로 저작권 문제는 없어진다.
- 다만 우리 서비스에는 업로드 페이지만 존재하게 된다.

### **4) 단순 정보, 기술 영상에 활용**

- 시각장애인으로도 타겟을 맞출 수 있음
- 보통 한 사람이 말하고 브금, 효과음이 적어서 손볼 부분이 줄어든다.
- 특정 분야에서만 사용되는 단어가 제대로 번역되지 않을 수 있다.

---

## 🔗 참고자료 & 유사 서비스

- [VOCO](https://cse.ewha.ac.kr/cse/academic/graduation-work.do?mode=view&articleNo=688427&article.offset=18&articleLimit=9)
- [HeyGen의 Video translate](https://jamake.io/ko/insight/131)
  - 번역 + TTS + 딥페이크까지 적용된 서비스

---

# 08/30 팀미팅

날짜: 2024년 8월 30일
상태: 완료

- 전문 용어 번역 처리
  - 예시 spring → 봄으로 바꿔야될듯
  - index, insert는 와닿지 않는거같음
- 카테고리 한정이 쫌 아쉽다

**특화를 포폴로 쓰기 위한 목적에서의 조언**

- 유튜버와 기술 한정이 조금 아쉽다 → 지루한 소재
- 유튜버나 IT기술말고 → 이 기술로 뭘 할래? 라고 했을때 컨님은 OTT(예능)에 적용할 것 같다
- K-예능 을 외국어로 할 수 있는 : 서진이네…
- FE 가 아쉽다? ㄴㄴ 페이지는 많지 않지만 사용자가 설정하는 부분을 처리하는 것이 일이 많아질 것
- 최소한의 사용자 입력을 받으면서 최대한의 효과를 낼 수 있는
- 실시간을 빼지 않고 MVP 구현 후 실시간 추가
- FE : 서비스 도입에 대한 화면 구성을 해도 된다
- 포괄적으로 만들면 오히려 유튜브를 연상하는데, 너무 한정적이면 지루할 수 있다
- 서비스에 대한 흥미의 스타트가 다르다
- 발음이 뭉개지는 사람은 우리도 틀리자 → 이 정도의 비즈니스 로직은 ㄱㅊ
- 정확도는 7~80%는 되지만 그 이상의 효과를 내는 서비스
- 너무 처음부터 정확한 발음을 타겟팅 하지 않아도 될 거 같다
- IT 는 영어로 듣는게 일반화 되어있는데, 굳이 한국어로?
- 지금은 기초 데이터가 없어도 생성형 AI가 만들어준다.
- 주니어 유튜버 섭외는 전략적인 면에서 고민
- 테크 유튜버를 기준으로 점점 넓혀나가자

# Question

- 저작권? : 저작권 나중에 산다고 하자! 이익 창출도 아니고 오히려 우리가 돈 받아야 됨
- 예능 말고 아나운서? : 는 좀 덜 흥미로울 수 있다…
- HeyGen → 직접 써보고 시장성 찾기: 우리는 무료로 해줍니다! 우리는 5분만에 해줍니다!
  - 직접 해 볼 필요가 있음. 유튜브 **뻥카**인 경우가 많다… **팩트체크** 필요!! 결과만 보지 마라
  - 딥페이크 까지?
- 원어민처럼 자연스럽지 않아서 원어를 보는게 아닐까?
  - 대충 번역 번역 이 아니라 최대한 자연스럽게 하는게 최대 핵심 비즈니스 로직
  - 살펴보자! 는 막자… 말투 후보정이 필요
- 뉴스? 더 수요가 없다…
- 연예인 라이브 방송 → 오히려 불쾌한 골짜기

# ETC

- 인공지능이 절대 사람을 이길 수 없는 것 → 사람은 직감이 있어서 내가 히브리어를 할 수 있는지 생각해보지 않고 히브리어를 할 수 없다고 바로 대답할 수 있다.
